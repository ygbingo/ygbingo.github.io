---
title: "-Python进程与线程最全指南"
date: 2025-06-10T13:54:44+08:00
draft: false
tags:
    - python
    - 设计模式
---

# threading - 基于线程的并行执行器

`threading`模块提供了一种在单个进程中并发运行多个**线程**（进程的较小单元）的方法。它允许创建和管理线程，从而能够并行执行任务并共享内存空间。当任务受限于I/O操作时，例如文件操作或发出网络请求，线程特别有用，因为在这些情况下，大部分时间都花在等待外部资源上。 

threading的一个典型用例包括管理一个工作线程池，这些线程可以并发处理多个任务。以下是使用Thread创建和启动线程的基本示例：
```python
import threading
import time

def crawl(link, delay=3):
    print(f"crawl started for {link}")
    time.sleep(delay)  # Blocking I/O (simulating a network request)
    print(f"crawl ended for {link}")

links = [
    "https://python.org",
    "https://docs.python.org",
    "https://peps.python.org",
]

# Start threads for each link
threads = []
for link in links:
    # Using `args` to pass positional arguments and `kwargs` for keyword arguments
    t = threading.Thread(target=crawl, args=(link,), kwargs={"delay": 2})
    threads.append(t)

# Start each thread
for t in threads:
    t.start()

# Wait for all threads to finish
for t in threads:
    t.join()
```

> concurrent.futures.ThreadPoolExecutor 提供了一个更高级别的接口，可将任务推送到后台线程，而不会阻塞调用线程的执行，同时仍能够在需要时检索其结果。
```python
import concurrent.futures
import urllib.request

URLS = ['http://www.foxnews.com/',
        'http://www.cnn.com/',
        'http://europe.wsj.com/',
        'http://www.bbc.co.uk/',
        'http://nonexistent-subdomain.python.org/']

# Retrieve a single page and report the URL and contents
def load_url(url, timeout):
    with urllib.request.urlopen(url, timeout=timeout) as conn:
        return conn.read()

# We can use a with statement to ensure threads are cleaned up promptly
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    # Start the load operations and mark each future with its URL
    future_to_url = {executor.submit(load_url, url, 60): url for url in URLS}
    for future in concurrent.futures.as_completed(future_to_url):
        url = future_to_url[future]
        try:
            data = future.result()
        except Exception as exc:
            print('%r generated an exception: %s' % (url, exc))
        else:
            print('%r page is %d bytes' % (url, len(data)))

```

> queue 为正在运行的线程之间交换数据提供了一个线程安全的接口。
```python
import threading
import queue
import time
import random

def worker():
    while not q.empty():
        try:
            task = q.get(timeout=2)  # 从队列中取出一个任务
            print(f"线程 {threading.current_thread().name} 正在处理任务: {task}")
            time.sleep(random.random())  # 模拟处理时间
            q.task_done()  # 告诉队列该任务已完成
        except queue.Empty:
            break

q = queue.Queue()

for i in range(10):
    q.put(f"任务_{i+1}")

threads = []
for i in range(3):  # 3个线程并发消费
    t = threading.Thread(target=worker, name=f"Worker-{i+1}")
    t.start()
    threads.append(t)

q.join()

print("所有任务已处理完毕。")
```
> asyncio 提供了一种实现任务级并发的替代方法，无需使用多个操作系统线程。

### CPython的实现细节
在CPython中，由于全局解释器锁的存在，一次只能有一个线程执行Python代码（尽管某些面向性能的库可能会克服这一限制）。如果你希望应用程序能更好地利用多核机器的计算资源，建议使用multiprocessing或concurrent.futures.ProcessPoolExecutor。不过，如果你想同时运行多个I/O密集型任务，多线程仍然是一种合适的模型。 

### 什么是GIL
CPython 解释器使用的一种机制，用于确保一次只有一个线程执行 Python 字节码。这通过使对象模型（包括如 dict 这样关键的内置类型）对并发访问隐式安全，简化了 CPython 的实现。锁定整个解释器使得解释器更易于实现多线程，但代价是牺牲了多处理器机器所提供的大部分并行性。 

然而，一些标准的或第三方的扩展模块被设计为在执行诸如压缩或哈希计算等计算密集型任务时释放全局解释器锁（GIL）。此外，在执行I/O操作时，GIL也总是会被释放。

与 multiprocessing 模块不同，multiprocessing使用单独的进程来绕过 全局解释器锁（GIL），而 threading 模块在单个进程内运行，这意味着所有线程共享相同的内存空间。然而，当涉及 CPU 密集型任务时，GIL 限制了线程的性能提升，因为一次只能有一个线程执行 Python 字节码。尽管如此，在许多场景中，线程仍然是实现并发的有用工具。 

> 从Python 3.13开始，可以使用--disable-gil构建配置禁用全局解释器锁（GIL）。使用此选项构建Python后，必须使用-X gil=0运行代码，或者在设置PYTHON_GIL=0环境变量后运行。此功能可提高多线程应用程序的性能，并使高效使用多核CPU变得更加容易。有关更多详细信息，请参阅PEP 703。 

# multiprocessing — 基于进程的并行处理

### 介绍
multiprocessing 是一个包，它支持使用类似于 threading 模块的 API 来创建进程。multiprocessing 包提供本地和远程并发，通过使用子进程而非线程有效地避开了 全局解释器锁。因此，multiprocessing 模块允许程序员在给定机器上充分利用多个处理器。它可在 POSIX 和 Windows 系统上运行。 

`multiprocessing`模块还引入了一些在`threading`模块中没有类似功能的API。其中一个主要示例是`Pool`对象，它提供了一种便捷的方式，可在多个输入值上并行执行函数，将输入数据分布到各个进程中（数据并行）。以下示例展示了在模块中定义此类函数的常见做法，以便子进程能够成功导入该模块。这个使用`Pool`进行数据并行的基本示例，
```python
from multiprocessing import Pool

def f(x):
    return x*x

if __name__ == '__main__':
    with Pool(5) as p:
        print(p.map(f, [1, 2, 3]))
```

> concurrent.futures.ProcessPoolExecutor 提供了一个更高级别的接口，可将任务推送到后台进程，而不会阻塞调用进程的执行。与直接使用 Pool 接口相比，concurrent.futures API 更易于将工作提交到基础进程池与等待结果的操作分离开来。

```python
import concurrent.futures
import math

PRIMES = [
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419]

def is_prime(n):
    if n < 2:
        return False
    if n == 2:
        return True
    if n % 2 == 0:
        return False

    sqrt_n = int(math.floor(math.sqrt(n)))
    for i in range(3, sqrt_n + 1, 2):
        if n % i == 0:
            return False
    return True

def main():
    with concurrent.futures.ProcessPoolExecutor() as executor:
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))

if __name__ == '__main__':
    main()
```

### Process
在multiprocessing中，进程是通过创建一个Process对象，然后调用其start()方法来生成的。Process遵循threading.Thread。
为了展示所涉及的各个进程ID，这里有一个扩展示例:
```python
from multiprocessing import Process
import os

def info(title):
    print(title)
    print('module name:', __name__)
    print('parent process:', os.getppid())
    print('process id:', os.getpid())

def f(name):
    info('function f')
    print('hello', name)

if __name__ == '__main__':
    info('main line')
    p = Process(target=f, args=('bob',))
    p.start()
    p.join()
```

### 上下文和启动方法
根据平台的不同，multiprocessing 支持三种启动进程的方式。这些 启动方法 分别是:

- spawn: 父进程启动一个全新的Python解释器进程。子进程将只继承运行进程对象的run()方法所需的那些资源。特别地，父进程中不必要的文件描述符和句柄不会被继承。与使用fork或forkserver相比，使用此方法启动进程相当慢。在POSIX和Windows平台上可用。在Windows和macOS上是默认方式。

- fork: 父进程使用os.fork()来派生Python解释器。子进程在启动时，实际上与父进程完全相同。父进程的所有资源都会被子进程继承。请注意，安全地派生多线程进程是有问题的。此方法在POSIX系统上可用。目前，除了macOS之外，在POSIX系统上这是默认设置。

> 注意: 在Python 3.14中，默认的启动方法将不再是fork。需要fork的代码应通过get_context() 或 set_start_method() 显式指定。 

- forkserver: 当程序启动并选择forkserver启动方法时，会生成一个服务器进程。从那时起，每当需要一个新进程时，父进程就会连接到该服务器并请求它派生一个新进程。除非系统库或预加载的导入作为副作用生成线程，否则fork服务器进程是单线程的，因此它使用os.fork()通常是安全的。不会继承不必要的资源。此方法在支持通过Unix管道传递文件描述符的POSIX平台（如Linux）上可用。

要选择一种启动方法，你需在主模块的set_start_method()子句中使用set_start_method()
```python
import multiprocessing as mp

def foo(q):
    q.put('hello')

if __name__ == '__main__':
    mp.set_start_method('spawn')
    q = mp.Queue()
    p = mp.Process(target=foo, args=(q,))
    p.start()
    print(q.get())
    p.join()
```

> set_start_method() 在程序中使用次数不应超过一次。
> 或者，你可以使用 get_context() 来获取一个上下文对象。上下文对象与 multiprocessing 模块具有相同的 API，并且允许在同一个程序中使用多种启动方法。

``` python
import multiprocessing as mp

def foo(q):
    q.put('hello')

if __name__ == '__main__':
    ctx = mp.get_context('spawn')
    q = ctx.Queue()
    p = ctx.Process(target=foo, args=(q,))
    p.start()
    print(q.get())
    p.join()
```

请注意，与一种上下文相关的对象可能与另一种上下文的进程不兼容。 特别是，使用fork上下文创建的锁，不能传递给使用spawn或forkserver启动方法启动的进程。 希望使用特定启动方法的库或许应该使用get_context()，以避免干扰库用户的选择。

> 注意：在POSIX系统上，'spawn'和'forkserver'启动方法通常不能与 “冻结的” 可执行文件（即由PyInstaller和cx_Freeze等软件包生成的二进制文件）一起使用。如果代码不使用线程，'fork'启动方法可能会起作用。 

### 进程之间通信
multiprocessing 支持进程间的两种通信通道：

1. Pipe（管道）
Pipe()函数返回一对通过管道连接的连接对象，默认情况下该管道是双工的（双向的）。由Pipe()返回的两个连接对象代表管道的两端。每个连接对象都有send()和recv()方法（以及其他方法）。请注意，如果两个进程（或线程）试图同时从管道的同一端读取或写入数据，管道中的数据可能会损坏。当然，同时使用管道不同端的进程不会有数据损坏的风险。 send() 方法将对象序列化，而recv() 方法重新创建该对象。

```python
from multiprocessing import Process, Pipe

def f(conn):
    conn.send([42, None, 'hello'])
    conn.close()

if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(child_conn,))
    p.start()
    print(parent_conn.recv())   # prints "[42, None, 'hello']"
    p.join()
```

2. Queue（队列）

```python
from multiprocessing import Process, Queue

def f(q):
    q.put([42, None, 'hello'])

if __name__ == '__main__':
    q = Queue()
    p = Process(target=f, args=(q,))
    p.start()
    print(q.get())    # prints "[42, None, 'hello']"
    p.join()
```

### 进程锁
multiprocessing 包含了 threading 中所有同步原语的等效物。例如，人们可以使用锁来确保一次只有一个进程向标准输出打印内容：
```python
from multiprocessing import Process, Lock

def f(l, i):
    l.acquire()
    try:
        print('hello world', i)
    finally:
        l.release()

if __name__ == '__main__':
    lock = Lock()

    for num in range(10):
        Process(target=f, args=(lock, num)).start()
```

### 进程间共享状态
先看下面的代码：
```python
from multiprocessing import Process

def f(n, a):
    n = 3.1415927
    for i in range(len(a)):
        a[i] = -a[i]

if __name__ == '__main__':
    num = 0
    arr = list(range(10))

    p = Process(target=f, args=(num, arr))
    p.start()
    p.join()

    print(num)  # 0
    print(arr[:])  # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
```
会发现f并没有真的修改num和arr的值，这是因为多进程是基于操作系统的，操作系统的进程之间是隔离的，所以修改num和arr的值，只是修改了进程的副本，而不是进程本身。

那么，如果要修改num和arr的值，怎么办呢？

multiprocessing提供了两种方案:
1. 共享内存:
```python
from multiprocessing import Process, Value, Array

def f(n, a):
    n.value = 3.1415927
    for i in range(len(a)):
        a[i] = -a[i]

if __name__ == '__main__':
    num = Value('d', 0.0)
    arr = Array('i', range(10))

    p = Process(target=f, args=(num, arr))
    p.start()
    p.join()

    print(num.value)
    print(arr[:])
```
2. 进程服务
```python
from multiprocessing import Process, Manager

def f(d, l):
    d[1] = '1'
    d['2'] = 2
    d[0.25] = None
    l.reverse()

if __name__ == '__main__':
    with Manager() as manager:
        d = manager.dict()
        l = manager.list(range(10))

        p = Process(target=f, args=(d, l))
        p.start()
        p.join()

        print(d)
        print(l)
```

### 线程池(推荐该方法)
Pool类代表一个工作进程池。它具有一些方法，允许以几种不同的方式将任务卸载到工作进程中。
```python
from multiprocessing import Pool, TimeoutError
import time
import os

def f(x):
    return x*x

if __name__ == '__main__':
    # start 4 worker processes
    with Pool(processes=4) as pool:

        # print "[0, 1, 4,..., 81]"
        print(pool.map(f, range(10)))

        # print same numbers in arbitrary order
        for i in pool.imap_unordered(f, range(10)):
            print(i)

        # evaluate "f(20)" asynchronously
        res = pool.apply_async(f, (20,))      # runs in *only* one process
        print(res.get(timeout=1))             # prints "400"

        # evaluate "os.getpid()" asynchronously
        res = pool.apply_async(os.getpid, ()) # runs in *only* one process
        print(res.get(timeout=1))             # prints the PID of that process

        # launching multiple evaluations asynchronously *may* use more processes
        multiple_results = [pool.apply_async(os.getpid, ()) for i in range(4)]
        print([res.get(timeout=1) for res in multiple_results])

        # make a single worker sleep for 10 seconds
        res = pool.apply_async(time.sleep, (10,))
        try:
            print(res.get(timeout=1))
        except TimeoutError:
            print("We lacked patience and got a multiprocessing.TimeoutError")

        print("For the moment, the pool remains available for more work")

    # exiting the 'with'-block has stopped the pool
    print("Now the pool is closed and no longer available")
```

使用```multiprocessing.Pool```时需要注意:
- 不能在 Windows 上或交互式环境（如 Jupyter Notebook）中直接使用匿名函数或 lambda
- Pool 默认启动的进程数是 CPU 核心数
- Pool 不能嵌套使用（Windows）
- apply_async 不是立即阻塞的，但 get() 是阻塞的
- Pool 对象必须关闭和 join，否则可能导致僵尸进程
- 全局变量和共享状态在 Pool 中可能不安全

# 总结
上面我们详细对比了Python中多线程和多进程的实现方式，由于该文档偏向对比，可能并非针对某个知识点深入讨论, 在真正使用时, 还是建议多多尝试, 适合自己的才是最好的。
